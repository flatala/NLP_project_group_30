{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGs for Open Domain Complex QA\n",
    "TU Delft, EEMCS, Natural Language Processing 2025, Group 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "Contents of this notebook.\n",
    "\n",
    "1. RAGs for Open Domain Complex QA\n",
    "    - Contents\n",
    "    - Setup\n",
    "        - 1.1 Dependencies\n",
    "        - 1.2 Imports\n",
    "        - 1.3 Preparing the general dataset \n",
    "        - 1.4 Setting up the Llama model for QA\n",
    "2. Experiments\n",
    "    - 2.1 Experiment 1\n",
    "    - 2.2 Experiment 2\n",
    "    - 2.3 Experiment 3\n",
    "    - 2.4 Experiment 4\n",
    "    - 2.5 Experiment 5\n",
    "\n",
    "3. ADORE\n",
    "    - 1. Imports\n",
    "    - 2. Load Data\n",
    "    - 3. Fine-tuning query encoder using ADORE\n",
    "    - 4. Evaluation of QA performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, install the dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% uv venv\n",
    "% source venv/bin/activate\n",
    "% uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD LIB\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# THIRD PARTY\n",
    "from bert_score import score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "\n",
    "# LOCAL LIB\n",
    "from utils import (\n",
    "    AdoreRetriever,\n",
    "    ContrieverRetriever,\n",
    "    plot_accuracy_bar_chart,\n",
    "    prepare_prompt,\n",
    "    get_answer_from_model_output,\n",
    "    exact_match_score,\n",
    "    cover_exact_match_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"HF_TOKEN\"] = \"<HF TOKEN HERE>\"\n",
    "\n",
    "config_path = \"config.ini\"\n",
    "\n",
    "loader = RetrieverDataset(\n",
    "    \"wikimultihopqa\", \"wikimultihopqa-corpus\", config_path, Split.DEV, tokenizer=None\n",
    ")\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "config_instance = DenseHyperParams(\n",
    "    query_encoder_path=\"facebook/contriever\",\n",
    "    document_encoder_path=\"facebook/contriever\",\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "contrvr_search: ContrieverRetriever = ContrieverRetriever(\n",
    "    config_instance, \"indices\", \"index_1\"\n",
    ")\n",
    "similarity_measure = CosineSimilarity()\n",
    "retriever_response = contrvr_search.retrieve(\n",
    "    corpus, queries, 100, similarity_measure, chunk=False, chunksize=400000\n",
    ")\n",
    "\n",
    "metrics = RetrievalMetrics(k_values=[1, 3, 5])  # Evaluate retrieval metrics\n",
    "print(metrics.evaluate_retrieval(qrels=qrels, results=retriever_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparing the general dataset based on retrieval results\n",
    "This dataset cotains the dev.json entries with questions and asnwers and all relevant documents as per the retriever. \n",
    "In combination with corpus_dict (use it for random samles or sth) this should be enough for the experiments up to adore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into kernel\n",
    "\n",
    "dataset_dir = \"data\"\n",
    "dev_path = f\"{dataset_dir}/musique/dev.json\"\n",
    "corpus_path = f\"{dataset_dir}/corpus/wiki_musique_corpus.json\"\n",
    "\n",
    "with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dev data by _id\n",
    "\n",
    "dev_dict = defaultdict(list)\n",
    "\n",
    "for item in dev_data:\n",
    "    if \"_id\" in item:\n",
    "        _id = item[\"_id\"]\n",
    "        dev_dict[_id].append(item)\n",
    "    else:\n",
    "        print(\"Warning: JSON object missing '_id' field:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from dev data and retrieved contexts\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "\n",
    "dataset = []  # a list of dictionaries where each represnts a question from the dev set and related retrieved contexts\n",
    "for dev_key, retrieved_contexts in islice(\n",
    "    retriever_response.items(), len(retriever_response.items())\n",
    "):\n",
    "    outer_dict = defaultdict(\n",
    "        list\n",
    "    )  # the outer dictionary with dev_id, dev_full, context_list\n",
    "    outer_dict[\"dev_id\"] = dev_key\n",
    "    dev_element = dev_dict[dev_key]\n",
    "    outer_dict[\"dev_full\"] = dev_element[0]\n",
    "\n",
    "    context_list = []  # list of dictionaries where each dict has the following keys: context_id, context_score, context_full\n",
    "\n",
    "    # contexts are ordered from the best to the worst, accoring to the used retriever\n",
    "    sorted_contexts = sorted(\n",
    "        retrieved_contexts.items(), key=lambda item: item[1], reverse=True\n",
    "    )\n",
    "    for context_key, context_score in sorted_contexts:\n",
    "        context_dict = defaultdict(list)\n",
    "        context_dict[\"context_id\"] = context_key\n",
    "        context_dict[\"context_score\"] = context_score\n",
    "        context_dict[\"context_full\"] = corpus_dict[context_key]\n",
    "        context_list.append(context_dict)\n",
    "\n",
    "    outer_dict[\"context_list\"] = context_list\n",
    "\n",
    "    dataset.append(outer_dict)\n",
    "\n",
    "print(\"Dataset created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Setting up the llama model for question answering (+ a usage example)\n",
    "\n",
    "First, we set up the llama model for question answering, with a BitsAndBytesConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# For any hf model to work just use AutoModelForCausalLM instead of LLamaForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 for A6000\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = transformers.pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "joe_biden_prompt = prepare_prompt(\n",
    "    question=\"Who directed movies called Example Movie?\",\n",
    "    evidences=[\n",
    "        \"Example Movie was directed by Joe Biden.\",\n",
    "        \"Example Movies is a thriller.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "t1 = time()\n",
    "response = text_generator(joe_biden_prompt, max_new_tokens=100)\n",
    "t2 = time()\n",
    "\n",
    "print(f\"\\nRESPONSE achieved in {t2 - t1}:.2f seconds...\\n\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer: str = get_answer_from_model_output(response)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Experiment 1\n",
    "\n",
    "In the cell below is the non multi threaded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "elem_count = 0\n",
    "count_parsing_errors = 0\n",
    "\n",
    "counts = [5, 15]\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "count_iters = 0\n",
    "\n",
    "for element in dataset_copy:\n",
    "    count_iters += 1\n",
    "    elem_count += 1\n",
    "\n",
    "    if count_iters == 100:\n",
    "        t_curr = time()\n",
    "        print(f\"\\nCompleted iterations: {elem_count}\")\n",
    "        print(f\"Time ellapsed: {t_curr - t_start}\\n\")\n",
    "        count_iters = 0\n",
    "\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    for count in counts:\n",
    "        t_count = time()\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "        output = text_generator(prompt, max_new_tokens=50)\n",
    "\n",
    "        try:\n",
    "            model_answer = get_answer_from_model_output(output)\n",
    "            if model_answer is None or correct_answer is None:\n",
    "                print(\n",
    "                    f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            EM_score = exact_match_score(model_answer, correct_answer)\n",
    "            cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "            P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "            bert_f1 = F1.mean().item()\n",
    "\n",
    "            bertscore_f1_scores[count] += bert_f1\n",
    "            exact_match_counts[count] += EM_score\n",
    "            exact_match_cover_counts[count] += cover_score\n",
    "        except JSONDecodeError:\n",
    "            count_parsing_errors += 1\n",
    "\n",
    "for count in counts:\n",
    "    bertscore_f1_scores[count] /= elem_count\n",
    "    exact_match_counts[count] /= elem_count\n",
    "    exact_match_cover_counts[count] /= elem_count\n",
    "\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_counts,\n",
    "    title=\"Exact Match Performance - Contriever Baseline\",\n",
    "    save_path=\"plots/contrv_baseline_exact_match_2.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_counts,\n",
    "    title=\"Cover Exact Match Performance - Contriever Baseline\",\n",
    "    save_path=\"plots/contrv_baseline_cover_exact_match_2.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_f1_scores,\n",
    "    title=\"Berstscore Performance - Contriever Baseline\",\n",
    "    save_path=\"plots/contrv_baseline_bertscore_2.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oracle_contexts(dev_data):\n",
    "    oracle_contexts = {}\n",
    "    for item in dev_data:\n",
    "        question_id = item[\"_id\"]\n",
    "        supporting_facts = item.get(\"supporting_facts\", [])\n",
    "        contexts = item.get(\"context\", [])\n",
    "\n",
    "        filtered_contexts = []\n",
    "        for fact in supporting_facts:\n",
    "            fact_title, _ = fact\n",
    "            for context_title, context_texts in contexts:\n",
    "                if context_title == fact_title:\n",
    "                    filtered_contexts.extend(context_texts)\n",
    "\n",
    "        oracle_contexts[question_id] = filtered_contexts\n",
    "\n",
    "    return oracle_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_contexts = load_oracle_contexts(dev_data)\n",
    "\n",
    "oracle_dataset = []\n",
    "\n",
    "for item in dev_data:\n",
    "    outer_dict = defaultdict(list)\n",
    "    outer_dict[\"dev_id\"] = item[\"_id\"]\n",
    "    outer_dict[\"dev_full\"] = item\n",
    "    outer_dict[\"context_list\"] = [\n",
    "        {\"text\": context} for context in oracle_contexts[item[\"_id\"]]\n",
    "    ]\n",
    "    oracle_dataset.append(outer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "oracle_exact_match_counts = defaultdict(int)\n",
    "oracle_cover_match_counts = defaultdict(int)\n",
    "oracle_bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "elem_count = 0\n",
    "count_parsing_errors = 0\n",
    "\n",
    "t_start = time()\n",
    "count_iters = 0\n",
    "\n",
    "oracle_dataset_copy = oracle_dataset[:1200]\n",
    "print(f\"Starting run on {len(oracle_dataset_copy)} entries...\")\n",
    "\n",
    "for element in oracle_dataset_copy:\n",
    "    count_iters += 1\n",
    "    elem_count += 1\n",
    "\n",
    "    if count_iters == 100:\n",
    "        t_curr = time()\n",
    "        print(f\"\\nCompleted iterations: {elem_count}\")\n",
    "        print(f\"Time ellapsed: {t_curr - t_start}\\n\")\n",
    "        count_iters = 0\n",
    "\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    question = element[\"dev_full\"][\"question\"]\n",
    "    correct_answer = element[\"dev_full\"][\"answer\"]\n",
    "    oracle_context_texts = [context[\"text\"] for context in element[\"context_list\"]]\n",
    "\n",
    "    prompt = prepare_prompt(question, oracle_context_texts)\n",
    "    output = text_generator(prompt, max_new_tokens=50)\n",
    "\n",
    "    try:\n",
    "        model_answer = get_answer_from_model_output(output)\n",
    "        if model_answer is None or correct_answer is None:\n",
    "            print(\n",
    "                f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        EM_score = exact_match_score(model_answer, correct_answer)\n",
    "        cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "        P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "        bert_f1 = F1.mean().item()\n",
    "\n",
    "        oracle_bertscore_f1_scores[1] += bert_f1\n",
    "        oracle_exact_match_counts[1] += EM_score\n",
    "        oracle_cover_match_counts[1] += cover_score\n",
    "\n",
    "    except JSONDecodeError:\n",
    "        # print(f'Error extracting answer for item {elem_count}')\n",
    "        # pprint(output)\n",
    "        count_parsing_errors += 1\n",
    "\n",
    "oracle_bertscore_f1_scores[1] /= elem_count\n",
    "oracle_exact_match_counts[1] /= elem_count\n",
    "oracle_cover_match_counts[1] /= elem_count\n",
    "\n",
    "pprint(oracle_exact_match_counts)\n",
    "pprint(oracle_cover_match_counts)\n",
    "print(f\"Parsing errors: {count_parsing_errors}\")\n",
    "print(\"Total elapsed time:\", time() - t_start)\n",
    "\n",
    "plot_accuracy_bar_chart(\n",
    "    oracle_exact_match_counts,\n",
    "    title=\"Exact Match Performance - Oracle Contexts\",\n",
    "    save_path=\"plots/oracle_exact_match_2.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    oracle_cover_match_counts,\n",
    "    title=\"Cover Exact Match Performance - Oracle Contexts\",\n",
    "    save_path=\"plots/oracle_cover_exact_match_2.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    oracle_bertscore_f1_scores,\n",
    "    title=\"Bert Score Performance - Oracle Contexts\",\n",
    "    save_path=\"plots/oracle_bertscore_2.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Experiment 3\n",
    "\n",
    "First let's try combining them in a 1:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in islice(corpus_dict.items(), 1):\n",
    "    print(key)\n",
    "    pprint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_documents(corpus_dict, exclude_titles, n):\n",
    "    filtered_documents = [\n",
    "        doc[\"text\"]\n",
    "        for doc in corpus_dict.values()\n",
    "        if doc[\"title\"] not in exclude_titles\n",
    "    ]\n",
    "    n = min(n, len(filtered_documents))\n",
    "    return random.sample(filtered_documents, n)\n",
    "\n",
    "\n",
    "def halve_or_one(n):\n",
    "    return 1 if n == 1 else n // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "elem_count = 0\n",
    "count_parsing_errors = 0\n",
    "\n",
    "counts = [1, 5, 15]\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "count_iters = 0\n",
    "\n",
    "for element in dataset_copy:\n",
    "    count_iters += 1\n",
    "    elem_count += 1\n",
    "\n",
    "    if count_iters == 100:\n",
    "        t_curr = time()\n",
    "        print(f\"\\nCompleted iterations: {elem_count}\")\n",
    "        print(f\"Time ellapsed: {t_curr - t_start}\\n\")\n",
    "        count_iters = 0\n",
    "\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    for count in counts:\n",
    "        t_count = time()\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "\n",
    "        selected_contexts_titles = []\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            selected_contexts_titles.append(context_title)\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        # sample random documents different than the selected ones and append to evidences\n",
    "        sampled_docs = sample_documents(corpus_dict, selected_contexts_titles, count)\n",
    "        evidences = [*evidences, *sampled_docs]\n",
    "\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "        output = text_generator(prompt, max_new_tokens=50)\n",
    "\n",
    "        try:\n",
    "            model_answer = get_answer_from_model_output(output)\n",
    "            if model_answer is None or correct_answer is None:\n",
    "                print(\n",
    "                    f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            EM_score = exact_match_score(model_answer, correct_answer)\n",
    "            cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "            P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "            bert_f1 = F1.mean().item()\n",
    "\n",
    "            bertscore_f1_scores[count] += bert_f1\n",
    "            exact_match_counts[count] += EM_score\n",
    "            exact_match_cover_counts[count] += cover_score\n",
    "        except JSONDecodeError:\n",
    "            count_parsing_errors += 1\n",
    "\n",
    "for count in counts:\n",
    "    bertscore_f1_scores[count] /= elem_count\n",
    "    exact_match_counts[count] /= elem_count\n",
    "    exact_match_cover_counts[count] /= elem_count\n",
    "\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_counts,\n",
    "    title=\"Exact Match Performance - Contriever + Random Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_1_to_1_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_counts,\n",
    "    title=\"Cover Exact Match Performance - Contriever + Random Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_1_to_1_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_f1_scores,\n",
    "    title=\"Bertscore Performance - Contriever + Random Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_1_to_1_bertscore.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:1 ratio + this should run a bit faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "counts = [1, 5, 15]\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "queries = []\n",
    "expected_answers = []\n",
    "dev_ids = []\n",
    "counts_mapping = []\n",
    "\n",
    "# Prepare dataset\n",
    "for element in dataset_copy:\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    for count in counts:\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "\n",
    "        selected_contexts_titles = []\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            selected_contexts_titles.append(context_title)\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        # Sample random documents different than the selected ones and append to evidences\n",
    "        random_doc_num = halve_or_one(count)\n",
    "        sampled_docs = sample_documents(\n",
    "            corpus_dict, selected_contexts_titles, random_doc_num\n",
    "        )\n",
    "        evidences.extend(sampled_docs)\n",
    "\n",
    "        # Prepare prompt\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "\n",
    "        queries.append(prompt)\n",
    "        expected_answers.append(correct_answer)\n",
    "        dev_ids.append(dev_id)\n",
    "        counts_mapping.append(count)\n",
    "\n",
    "# Generate responses in batch\n",
    "outputs = text_generator(queries, max_new_tokens=50)\n",
    "\n",
    "# Evaluate responses\n",
    "count_parsing_errors = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    count = counts_mapping[i]\n",
    "    correct_answer = expected_answers[i]\n",
    "\n",
    "    try:\n",
    "        model_answer = get_answer_from_model_output(output)\n",
    "        if model_answer is None or correct_answer is None:\n",
    "            print(\n",
    "                f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        EM_score = exact_match_score(model_answer, correct_answer)\n",
    "        cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "        P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "        bert_f1 = F1.mean().item()\n",
    "\n",
    "        bertscore_f1_scores[count] += bert_f1\n",
    "        exact_match_counts[count] += EM_score\n",
    "        exact_match_cover_counts[count] += cover_score\n",
    "    except JSONDecodeError:\n",
    "        count_parsing_errors += 1\n",
    "\n",
    "# Normalize scores\n",
    "num_entries = len(dataset_copy)\n",
    "for count in counts:\n",
    "    bertscore_f1_scores[count] /= elem_count\n",
    "    exact_match_counts[count] /= elem_count\n",
    "    exact_match_cover_counts[count] /= elem_count\n",
    "\n",
    "# Print results\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "\n",
    "# Plot results\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_counts,\n",
    "    title=\"Exact Match Performance - Contriever + Random Contexts (2:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_2_to_1_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_counts,\n",
    "    title=\"Cover Exact Match Performance - Contriever + Random Contexts (2:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_2_to_1_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_f1_scores,\n",
    "    title=\"Bertscore Performance - Contriever + Random Contexts (2:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_random_2_to_1_bertscore.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Experiment 4: Hard Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def sample_hard_negative_documents(\n",
    "    query, corpus_dict, exclude_titles, n, embedding_model\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample top n hard negatives by similarity score but exclude ground truths.\n",
    "\n",
    "    Parameters:\n",
    "    - query: str, the query for which to sample hard negatives\n",
    "    - corpus_dict: dict, where keys are titles and values are texts\n",
    "    - exclude_titles: set or list, titles to exclude from consideration\n",
    "    - n: int, number of hard negatives to sample\n",
    "    - embedding_model: an embedding model with `.encode()` method to generate embeddings\n",
    "\n",
    "    Returns:\n",
    "    - List of top n hard negatives by similarity score (texts only)\n",
    "    \"\"\"\n",
    "    # Filter documents to exclude ground truth titles\n",
    "    filtered_documents = [\n",
    "        text for title, text in corpus_dict.items() if title not in exclude_titles\n",
    "    ]\n",
    "    if len(filtered_documents) == 0:\n",
    "        return []\n",
    "\n",
    "    # Compute embeddings for the query and the candidate documents\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    doc_embeddings = embedding_model.encode(filtered_documents, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = util.cos_sim(query_embedding, doc_embeddings).squeeze()\n",
    "    # print(similarities)\n",
    "    # Sort documents by similarity in descending order\n",
    "    sorted_indices = np.argsort(-similarities.cpu().numpy())\n",
    "    top_n_indices = sorted_indices[: min(n, len(filtered_documents))]\n",
    "    # print(similarities[top_n_indices[0]])\n",
    "    # Select the top n hard negatives\n",
    "    # print([filtered_documents[idx] for idx in sorted_indices[:5]])\n",
    "    hard_negatives = [filtered_documents[idx] for idx in top_n_indices]\n",
    "    return hard_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Printing to clarify which documents to consider, what to take etc, understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "\n",
    "counts = [1, 5, 15]\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "print(\"len of corpus dict\", len(corpus_dict))\n",
    "print(\"Example of 1 corpus dict k,v pair\")\n",
    "iter = 1\n",
    "for k, v in corpus_dict.items():\n",
    "    # print(f\"Key: {k}, Value: {v}\\n\\n\")\n",
    "    iter += 1\n",
    "    break\n",
    "print(corpus_dict[\"126271\"])\n",
    "queries = []\n",
    "expected_answers = []\n",
    "dev_ids = []\n",
    "counts_mapping = []\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Prepare dataset\n",
    "for element in dataset_copy:\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "    dev_ctxt_titles = []\n",
    "    all_titles = []\n",
    "    result_dict = defaultdict()\n",
    "    # Union of all_contexts and dev_dict context into 1 dictionary\n",
    "    # Process `all_contexts`\n",
    "    for context in all_contexts:\n",
    "        title = context[\"context_full\"][\"title\"]\n",
    "        text = context[\"context_full\"][\"text\"]\n",
    "        result_dict[title] = text\n",
    "    # Process `dev_dict['context']`\n",
    "    for entry in dev_dict[\"context\"]:\n",
    "        title = entry[0]\n",
    "        text = \" \".join(entry[1])  # Join the list of strings into a single text\n",
    "        result_dict[title] = text\n",
    "    ground_truth_titles = [elem[0] for elem in dev_dict[\"supporting_facts\"]]\n",
    "    for i in range(len(dev_dict[\"context\"])):\n",
    "        dev_ctxt_titles.append(dev_dict[\"context\"][i][0])\n",
    "    for context_item in all_contexts:\n",
    "        all_titles.append(context_item[\"context_full\"][\"title\"])\n",
    "    print(\"Everything below is for 1 example, i.e, 1 of the 1200 samples.\\n\")\n",
    "    element_keys = [k for k, v in element.items()]\n",
    "    print(\"The keys for every element:\", element_keys, \"\\n\")\n",
    "    print(\n",
    "        \"Length of the context list (superset, all_contexts=dataset_element[context_list]:\",\n",
    "        len(all_contexts),\n",
    "    )\n",
    "    print(\n",
    "        \"Example of one of the \",\n",
    "        len(all_contexts),\n",
    "        \" all_contexts elements:\\n\",\n",
    "        all_contexts[0],\n",
    "        \"\\n\",\n",
    "    )\n",
    "    dev_dict_keys = [k for k, v in dev_dict.items()]\n",
    "    print(\"The keys in (dev_dict) dataset_element[dev_full]:\", dev_dict_keys)\n",
    "    print(\n",
    "        \"dev_dict[context] length and type:\",\n",
    "        len(dev_dict[\"context\"]),\n",
    "        type(dev_dict[\"context\"]),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    print(\"One example of dev_dict[context]:\", dev_dict[\"context\"][:2], \"\\n\")\n",
    "    print(\"List of all titles in dev_dict[context]\", set(dev_ctxt_titles), \"\\n\")\n",
    "    print(\"The question we have is: \", question)\n",
    "    print(\n",
    "        \"List of all elements of dev_dict[supporting_facts], considered to be the ground truth :\",\n",
    "        dev_dict[\"supporting_facts\"],\n",
    "    )\n",
    "    print(\"List of ground truth titles: \", ground_truth_titles)\n",
    "\n",
    "    for count in counts:\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "\n",
    "        selected_contexts_titles = []\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            selected_contexts_titles.append(context_title)\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        # Sample random documents different than the selected ones and append to evidences\n",
    "        # hard_neg_doc_num = halve_or_one(count)\n",
    "        # sampled_docs = sample_documents(corpus_dict, selected_contexts_titles, random_doc_num)\n",
    "        sampled_docs = sample_hard_negative_documents(\n",
    "            question, result_dict, selected_contexts_titles, count, embedding_model\n",
    "        )\n",
    "        evidences.extend(sampled_docs)\n",
    "\n",
    "        # Prepare prompt\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "\n",
    "        queries.append(prompt)\n",
    "        expected_answers.append(correct_answer)\n",
    "        dev_ids.append(dev_id)\n",
    "        counts_mapping.append(count)\n",
    "\n",
    "    iter = iter + 1\n",
    "    if iter > 1:\n",
    "        break\n",
    "# print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Top-k contriever + hard_negatives from (all_contexts U dev_dict) (1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "counts = [1, 5, 15]\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "queries = []\n",
    "expected_answers = []\n",
    "dev_ids = []\n",
    "counts_mapping = []\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Prepare dataset\n",
    "iter = 1\n",
    "for element in dataset_copy:\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    ground_truth_titles = [elem[0] for elem in dev_dict[\"supporting_facts\"]]\n",
    "\n",
    "    for count in counts:\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "        selected_contexts_titles = []\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            selected_contexts_titles.append(context_title)\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        # Sample the hard negatives\n",
    "        num_hard_neg = len(selected_contexts_titles)\n",
    "        selected_contexts_titles.extend(\n",
    "            ground_truth_titles\n",
    "        )  # Ground truth + top k titles\n",
    "        sampled_docs = []\n",
    "        hard_neg_so_far = 0\n",
    "        # print(len(evidences), evidences)\n",
    "        for context_item in all_contexts:\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            if context_title not in selected_contexts_titles:\n",
    "                sampled_docs.append(context_full[\"text\"])\n",
    "                hard_neg_so_far += 1\n",
    "                if hard_neg_so_far >= num_hard_neg:\n",
    "                    break\n",
    "        evidences.extend(sampled_docs)\n",
    "        # print(len(sampled_docs),sampled_docs)\n",
    "\n",
    "        # Prepare prompt\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "\n",
    "        queries.append(prompt)\n",
    "        expected_answers.append(correct_answer)\n",
    "        dev_ids.append(dev_id)\n",
    "        counts_mapping.append(count)\n",
    "    iter = iter + 1\n",
    "    if iter % 400 == 0:\n",
    "        print(iter)\n",
    "# print(queries)\n",
    "# Generate responses in batch\n",
    "outputs = text_generator(queries, max_new_tokens=50)\n",
    "print(\"outputs generated\")\n",
    "# Evaluate responses\n",
    "count_parsing_errors = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    count = counts_mapping[i]\n",
    "    correct_answer = expected_answers[i]\n",
    "\n",
    "    try:\n",
    "        model_answer = get_answer_from_model_output(output)\n",
    "        if model_answer is None or correct_answer is None:\n",
    "            print(\n",
    "                f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        EM_score = exact_match_score(model_answer, correct_answer)\n",
    "        cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "        P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "        bert_f1 = F1.mean().item()\n",
    "\n",
    "        bertscore_f1_scores[count] += bert_f1\n",
    "        exact_match_counts[count] += EM_score\n",
    "        exact_match_cover_counts[count] += cover_score\n",
    "    except JSONDecodeError:\n",
    "        # print(\"Erroneous output\",output)\n",
    "        # print(\"Correct\",correct_answer)\n",
    "        # break\n",
    "        count_parsing_errors += 1\n",
    "\n",
    "# Normalize scores\n",
    "num_entries = len(dataset_copy)\n",
    "for count in counts:\n",
    "    bertscore_f1_scores[count] /= elem_count\n",
    "    exact_match_counts[count] /= elem_count\n",
    "    exact_match_cover_counts[count] /= elem_count\n",
    "\n",
    "# Print results\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "\n",
    "# Plot results\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_counts,\n",
    "    title=\"Exact Match Performance - Contriever + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_neg_1_to_1_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_counts,\n",
    "    title=\"Cover Exact Match Performance - Contriever + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_neg_1_to_1_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_f1_scores,\n",
    "    title=\"Bertscore Performance - Contriever + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/contrv_and_neg_1_to_1_bertscore.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Ground truth (supporting facts only) + hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "\n",
    "exact_match_counts = 0.0\n",
    "exact_match_cover_counts = 0.0\n",
    "bertscore_f1_scores = 0.0\n",
    "\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "queries = []\n",
    "expected_answers = []\n",
    "dev_ids = []\n",
    "counts_mapping = []\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Prepare dataset\n",
    "iter = 1\n",
    "for element in dataset_copy:\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    ground_truth_titles = [elem[0] for elem in dev_dict[\"supporting_facts\"]]\n",
    "    evidences = []\n",
    "    for context in dev_dict[\"context\"]:\n",
    "        if context[0] in ground_truth_titles:\n",
    "            evidences.append(\" \".join(context[1]))\n",
    "\n",
    "    # Sample the hard negatives\n",
    "    num_hard_neg = len(ground_truth_titles)\n",
    "    sampled_docs = []\n",
    "    hard_neg_so_far = 0\n",
    "    # print(len(evidences), evidences)\n",
    "    for context_item in all_contexts:\n",
    "        context_full = context_item[\"context_full\"]\n",
    "        context_title = context_full[\"title\"]\n",
    "        if context_title not in ground_truth_titles:\n",
    "            sampled_docs.append(context_full[\"text\"])\n",
    "            hard_neg_so_far += 1\n",
    "            if hard_neg_so_far >= num_hard_neg:\n",
    "                break\n",
    "\n",
    "    evidences.extend(sampled_docs)\n",
    "    # Prepare prompt\n",
    "    prompt = prepare_prompt(question, evidences)\n",
    "\n",
    "    queries.append(prompt)\n",
    "    expected_answers.append(correct_answer)\n",
    "    dev_ids.append(dev_id)\n",
    "    # counts_mapping.append(count)\n",
    "    iter = iter + 1\n",
    "    if iter % 400 == 0:\n",
    "        print(iter)\n",
    "# Generate responses in batch\n",
    "outputs = text_generator(queries, max_new_tokens=50)\n",
    "\n",
    "# Evaluate responses\n",
    "count_parsing_errors = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    # count = counts_mapping[i]\n",
    "    correct_answer = expected_answers[i]\n",
    "\n",
    "    try:\n",
    "        model_answer = get_answer_from_model_output(output)\n",
    "        if model_answer is None or correct_answer is None:\n",
    "            print(\n",
    "                f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        EM_score = exact_match_score(model_answer, correct_answer)\n",
    "        cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "        P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "        bert_f1 = F1.mean().item()\n",
    "\n",
    "        bertscore_f1_scores += bert_f1\n",
    "        exact_match_counts += EM_score\n",
    "        exact_match_cover_counts += cover_score\n",
    "    except JSONDecodeError:\n",
    "        count_parsing_errors += 1\n",
    "\n",
    "# Normalize scores\n",
    "num_entries = len(dataset_copy)\n",
    "bertscore_f1_scores /= num_entries\n",
    "exact_match_counts /= num_entries\n",
    "exact_match_cover_counts /= num_entries\n",
    "\n",
    "# Print results\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "exact_match_dict = {\"2*|ground truth contexts|\": exact_match_counts}\n",
    "exact_match_cover_dict = {\"2*|ground truth contexts|\": exact_match_cover_counts}\n",
    "bertscore_dict = {\"2*|ground truth contexts|\": bertscore_f1_scores}\n",
    "# Plot results\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_dict,\n",
    "    title=\"Exact Match Performance - ground truth + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/groundtruth_and_neg_1_to_1_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_dict,\n",
    "    title=\"Cover Exact Match Performance - ground truth + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/groundtruth_and_neg_1_to_1_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_dict,\n",
    "    title=\"Bertscore Performance - ground truth + Hard Negative Contexts (1:1 ratio)\",\n",
    "    save_path=\"plots/groundtruth_and_neg_1_to_1_bertscore.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Experiment 5 : Oracle + remaining dev contexts baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "exact_match_counts = 0.0\n",
    "exact_match_cover_counts = 0.0\n",
    "bertscore_f1_scores = 0.0\n",
    "\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "queries = []\n",
    "expected_answers = []\n",
    "dev_ids = []\n",
    "counts_mapping = []\n",
    "\n",
    "# Prepare dataset\n",
    "iter = 1\n",
    "for element in dataset_copy:\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    evidences = []\n",
    "    selected_contexts_titles = []\n",
    "    for context in dev_dict[\"context\"]:\n",
    "        selected_contexts_titles.append(context[0])\n",
    "        evidences.append(\" \".join(context[1]))\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = prepare_prompt(question, evidences)\n",
    "\n",
    "    queries.append(prompt)\n",
    "    expected_answers.append(correct_answer)\n",
    "    dev_ids.append(dev_id)\n",
    "    # counts_mapping.append(count)\n",
    "    iter = iter + 1\n",
    "    if iter % 100 == 0:\n",
    "        print(iter)\n",
    "# print(queries)\n",
    "# Generate responses in batch\n",
    "outputs = text_generator(queries, max_new_tokens=50)\n",
    "\n",
    "# Evaluate responses\n",
    "count_parsing_errors = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    # count = counts_mapping[i]\n",
    "    correct_answer = expected_answers[i]\n",
    "\n",
    "    try:\n",
    "        model_answer = get_answer_from_model_output(output)\n",
    "        if model_answer is None or correct_answer is None:\n",
    "            print(\n",
    "                f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        EM_score = exact_match_score(model_answer, correct_answer)\n",
    "        cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "        P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "        bert_f1 = F1.mean().item()\n",
    "\n",
    "        bertscore_f1_scores += bert_f1\n",
    "        exact_match_counts += EM_score\n",
    "        exact_match_cover_counts += cover_score\n",
    "    except JSONDecodeError:\n",
    "        count_parsing_errors += 1\n",
    "\n",
    "# Normalize scores\n",
    "num_entries = len(dataset_copy)\n",
    "\n",
    "bertscore_f1_scores /= num_entries\n",
    "exact_match_counts /= num_entries\n",
    "exact_match_cover_counts /= num_entries\n",
    "\n",
    "# Print results\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "exact_match_dict = {\"|dev.json context list|\": exact_match_counts}\n",
    "exact_match_cover_dict = {\"|dev.json context list|\": exact_match_cover_counts}\n",
    "bertscore_dict = {\"|dev.json context list|\": bertscore_f1_scores}\n",
    "# Plot results\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_dict,\n",
    "    title=\"Exact Match Performance - Dev.json contexts\",\n",
    "    save_path=\"plots/dev_contexts_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_dict,\n",
    "    title=\"Cover Exact Match Performance - Dev.json contexts\",\n",
    "    save_path=\"plots/dev_contexts_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_dict,\n",
    "    title=\"Bertscore Performance - Dev.json contexts\",\n",
    "    save_path=\"plots/dev_contexts_bertscore.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADORE\n",
    "\n",
    "We have data for dev test and training, called dev.json, test.json and train.json. they have formats: dict_keys(['_id', 'type', 'question', 'context', 'supporting_facts', 'evidences', 'answer']). We build an ADORE implementation in the `Retriever` base class, called the `AdoreRetriever`. Specifically, we train a retrieval model using the **ADORE method**. We are going to compare it to the above implementation of the `HfRetriever`. We are going to use the `HfRetriever` as a base, the changes will be specified later. We will use a different `self.question_encoder`, i.e. our own trained model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Make sure that either you ran all imports at the top of the notebook or you run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD LIB\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# THIRD PARTY\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "\n",
    "\n",
    "# LOCAL LIB\n",
    "from utils import AdoreRetriever, ContrieverRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We load corpus and training data into the runtime. The dev/train/test.json files have formats: `dict_keys(['_id', 'type', 'question', 'context', 'supporting_facts', 'evidences', 'answer'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"data\"\n",
    "dev_path = f\"{dataset_dir}/musique/dev.json\"\n",
    "test_path = f\"{dataset_dir}/musique/test.json\"\n",
    "train_path = f\"{dataset_dir}/musique/train.json\"\n",
    "corpus_path = f\"{dataset_dir}/corpus/wiki_musique_corpus.json\"\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_dict = json.load(f)\n",
    "\n",
    "with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare a dataset with train set ids and respective positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_positives_full(dev_data):\n",
    "    oracle_contexts = {}\n",
    "    for item in dev_data:\n",
    "        question_id = item[\"_id\"]\n",
    "        supporting_facts = item.get(\"supporting_facts\", [])\n",
    "        contexts = item.get(\"context\", [])\n",
    "\n",
    "        filtered_contexts = []\n",
    "        for fact in supporting_facts:\n",
    "            fact_title, _ = fact\n",
    "            for context_title, context_texts in contexts:\n",
    "                if context_title == fact_title:\n",
    "                    filtered_contexts.append(\n",
    "                        {\"title\": context_title, \"text\": context_texts}\n",
    "                    )\n",
    "\n",
    "        q_c_dict = {\"question\": item[\"question\"], \"positives\": filtered_contexts}\n",
    "        oracle_contexts[question_id] = q_c_dict\n",
    "\n",
    "    return oracle_contexts\n",
    "\n",
    "\n",
    "positives_dict = load_positives_full(dev_data)\n",
    "\n",
    "print(type(positives_dict))\n",
    "\n",
    "for el in positives_dict:\n",
    "    pprint(el)\n",
    "    context_data = positives_dict[el]\n",
    "    pprint(context_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning the query encoder following the ADORE approach\n",
    "\n",
    "Here, we train the query_encoder using the ADORE method and the training data. Reminder, we start with the same model as the `HfRetriever`, the `facebook/contriever` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"HF_TOKEN\"] = \"<HF TOKEN HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ADORE training data formatting\n",
    "\n",
    "To use the data for training the query encoder using ADORE, we first need a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positive_negative_pairs(contexts: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Given a list of contexts where each context is a dictionary containing:\n",
    "        {\n",
    "            \"is_positive\": bool,\n",
    "            \"text\": str,\n",
    "            \"index\": int,\n",
    "            ... (other optional fields)\n",
    "        },\n",
    "    produce a list of all (positive, negative) context pairs WITHOUT duplicates.\n",
    "\n",
    "    Each item in the returned list has the form:\n",
    "        {\n",
    "            \"positive_text\": str,\n",
    "            \"negative_text\": str,\n",
    "            \"positive_index\": int,\n",
    "            \"negative_index\": int\n",
    "        }\n",
    "\n",
    "    'No duplicates' means we won't include the same text pair twice.\n",
    "    You can change the definition of 'duplicate' if needed (for example,\n",
    "    using the context 'index' instead of 'text').\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate contexts based on 'is_positive'\n",
    "    positive_contexts = [c for c in contexts if c.get(\"is_positive\", False)]\n",
    "    negative_contexts = [c for c in contexts if not c.get(\"is_positive\", False)]\n",
    "\n",
    "    # We'll keep track of pairs we've already seen so we don't repeat them\n",
    "    seen_pairs = set()\n",
    "    results = []\n",
    "\n",
    "    for pos_ctx in positive_contexts:\n",
    "        for neg_ctx in negative_contexts:\n",
    "            # Identify a pair by its texts (or by indices if that's preferable)\n",
    "            pair_identifier = (pos_ctx[\"text\"], neg_ctx[\"text\"])\n",
    "\n",
    "            # If we haven't seen this pair yet, add it to the results\n",
    "            if pair_identifier not in seen_pairs:\n",
    "                seen_pairs.add(pair_identifier)\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"positive_text\": pos_ctx[\"text\"],\n",
    "                        \"negative_text\": neg_ctx[\"text\"],\n",
    "                        \"positive_index\": pos_ctx[\"index\"],\n",
    "                        \"negative_index\": neg_ctx[\"index\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def prepare_positives_negatives_dataset(\n",
    "    positives_dict: dict, retriever_response: dict, corpus_dict: dict\n",
    ") -> list[dict]:\n",
    "    training_instances = []\n",
    "    for dev_key, retrieved_contexts in islice(\n",
    "        retriever_response.items(), len(retriever_response.items())\n",
    "    ):\n",
    "        positives_dict_item = positives_dict[dev_key]\n",
    "        question = positives_dict_item[\"question\"]\n",
    "\n",
    "        # key: str, value: str|list keys: text, content. for key=content, type(val)==list[str]\n",
    "        positives = positives_dict_item[\"positives\"]\n",
    "        positives_titles = []\n",
    "        for positive in positives:\n",
    "            title = positive[\"title\"]\n",
    "            positives_titles.append(title)\n",
    "\n",
    "        question_contexts = []\n",
    "\n",
    "        sorted_contexts = sorted(\n",
    "            retrieved_contexts.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "        for i, (context_key, context_score) in enumerate(sorted_contexts):\n",
    "            context_dict = defaultdict(list)\n",
    "            context_dict[\"context_id\"] = context_key\n",
    "            context_dict[\"context_score\"] = context_score\n",
    "            context_full = corpus_dict[context_key]\n",
    "\n",
    "            # print(context_score)\n",
    "            # pprint(context_full)\n",
    "\n",
    "            context_title = context_full[\"title\"]\n",
    "            context_text = context_full[\"text\"]\n",
    "\n",
    "            is_positive = context_title in positives_titles\n",
    "\n",
    "            question_contexts.append(\n",
    "                {\n",
    "                    \"is_positive\": is_positive,\n",
    "                    \"index\": i,\n",
    "                    \"title\": context_title,\n",
    "                    \"text\": context_text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        training_instances.append({\"question\": question, \"contexts\": question_contexts})\n",
    "\n",
    "    return training_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ADORE loss and helpers\n",
    "\n",
    "First, some helper functions that we need to calculate the ADORE loss. These are for the Mean Average Precision (or MAP) and the Delta_MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map_at_k_using_is_positive(ctx_list, top_k=10):\n",
    "    if not ctx_list:\n",
    "        return 0.0\n",
    "\n",
    "    average_precisions = []\n",
    "    relevant_count = 0\n",
    "\n",
    "    for i, c in enumerate(ctx_list):\n",
    "        if i >= top_k:\n",
    "            break\n",
    "        if c.get(\"is_positive\", False):\n",
    "            relevant_count += 1\n",
    "            precision_at_i = relevant_count / (i + 1)\n",
    "            average_precisions.append(precision_at_i)\n",
    "\n",
    "    if not average_precisions:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "\n",
    "def calculate_delta_map_using_is_positive(contexts, idx1, idx2):\n",
    "    max_index = len(contexts) - 1\n",
    "    if not (0 <= idx1 <= max_index) or not (0 <= idx2 <= max_index):\n",
    "        raise ValueError(\n",
    "            f\"Indices must be within [0..{max_index}], but got {idx1} and {idx2}.\"\n",
    "        )\n",
    "\n",
    "    original_map = calculate_map_at_k_using_is_positive(contexts, top_k=10)\n",
    "\n",
    "    modified_contexts = contexts.copy()\n",
    "    modified_contexts[idx1], modified_contexts[idx2] = (\n",
    "        modified_contexts[idx2],\n",
    "        modified_contexts[idx1],\n",
    "    )\n",
    "\n",
    "    swapped_map = calculate_map_at_k_using_is_positive(modified_contexts, top_k=10)\n",
    "\n",
    "    Delta_M_abs_value = abs(swapped_map - original_map)\n",
    "    return Delta_M_abs_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here, the actual ADORE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adore_loss(\n",
    "    f_q_d_pos: torch.Tensor, f_q_d_neg: torch.Tensor, delta_M: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ADORE loss from Eq. (15). (adjuste to usse MAP@10) Multiplies the standard pairwise RankNet loss\n",
    "    by the magnitude of the change in MAP@10 (Delta_M).\n",
    "    \"\"\"\n",
    "    return delta_M * torch.log(1 + torch.exp(f_q_d_neg - f_q_d_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Retriever Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.ini\"\n",
    "\n",
    "loader = RetrieverDataset(\n",
    "    \"wikimultihopqa\", \"wikimultihopqa-corpus\", config_path, Split.DEV, tokenizer=None\n",
    ")\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "print(f\"Loader initialized with {len(queries)} queries and {len(corpus)} documents.\")\n",
    "\n",
    "adore_config = DenseHyperParams(  # use a patched config for adore retriever\n",
    "    query_encoder_path=\"facebook/contriever\",  # we replace the query encoder later by our own\n",
    "    document_encoder_path=\"facebook/contriever\",\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Adore config: \", adore_config)\n",
    "\n",
    "# create the retriever instance, with the same config as the contriever.\n",
    "adore_retriever: AdoreRetriever = (\n",
    "    AdoreRetriever(  # NOTE: this is the same INDEX that we\n",
    "        config=adore_config,  # used for the contriever. But, they use the same\n",
    "        corpus_folder=\"indices\",  # corpus, and the same document encoder.\n",
    "        corpus_file=\"index_1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ADORE Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adore_query_encoder = AutoModel.from_pretrained(\n",
    "    \"facebook/contriever\"\n",
    ").cuda()  # We will finetune this query encoder !\n",
    "adore_tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n",
    "\n",
    "# training loop settings\n",
    "batch_size = 1200\n",
    "epochs = 10\n",
    "\n",
    "# checkpoint settings\n",
    "# save_every_n_batches = None # to save some disk space\n",
    "save_every_n_batches = 1000\n",
    "checkpoint_dir = \"checkpoints2\"  # define the dir for the checkpoints\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "TOP_K = 10\n",
    "for epoch in tqdm(range(epochs), desc=\"Training epoch: \"):\n",
    "    # optimizer - values based on the original paper.\n",
    "    optimizer = optim.AdamW(adore_query_encoder.parameters(), lr=5e-2)\n",
    "\n",
    "    # Adore retriever has two separate encoder, we update the query encoder only\n",
    "    retrieved_evidence = adore_retriever.retrieve(\n",
    "        corpus=corpus,\n",
    "        queries=queries,\n",
    "        top_k=TOP_K,\n",
    "        score_function=CosineSimilarity(),\n",
    "        qrels=qrels,\n",
    "    )\n",
    "\n",
    "    train_data: list[dict[str, str | list[dict]]] = prepare_positives_negatives_dataset(\n",
    "        positives_dict, retrieved_evidence, corpus_dict\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for i in tqdm(range(0, len(train_data), batch_size), desc=\"Training batch:\"):\n",
    "        batch = train_data[i : i + batch_size]\n",
    "\n",
    "        # define arrays\n",
    "        query_embeddings = []\n",
    "        positive_embeddings = []\n",
    "        negative_embeddings = []\n",
    "        m_abs_values = []\n",
    "\n",
    "        for elem in tqdm(batch, desc=\"Element in batch...: \"):\n",
    "            question = elem.get(\"question\")\n",
    "\n",
    "            # encode questions\n",
    "            question_tokens = adore_tokenizer(\n",
    "                question, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            query_encoding = adore_query_encoder(\n",
    "                **question_tokens\n",
    "            ).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            question_contexts = elem.get(\"contexts\")\n",
    "            pos_neg_pairs = generate_positive_negative_pairs(question_contexts)\n",
    "            for pos_neg in pos_neg_pairs:\n",
    "                pos_text = pos_neg.get(\"positive_text\")\n",
    "                neg_text = pos_neg.get(\"negative_text\")\n",
    "\n",
    "                # encode positive\n",
    "                pos_tokens = adore_tokenizer(\n",
    "                    pos_text, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pos_encoding = adore_retriever.context_encoder(\n",
    "                        **pos_tokens\n",
    "                    ).last_hidden_state.mean(dim=1)\n",
    "\n",
    "                # encode negative\n",
    "                neg_tokens = adore_tokenizer(\n",
    "                    neg_text, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    neg_encoding = adore_retriever.context_encoder(\n",
    "                        **neg_tokens\n",
    "                    ).last_hidden_state.mean(dim=1)\n",
    "\n",
    "                pos_idx = pos_neg.get(\"positive_index\")\n",
    "                neg_idx = pos_neg.get(\"negative_index\")\n",
    "                M_abs = calculate_delta_map_using_is_positive(\n",
    "                    contexts=question_contexts, idx1=pos_idx, idx2=neg_idx\n",
    "                )\n",
    "\n",
    "                query_embeddings.append(query_encoding)\n",
    "                positive_embeddings.append(pos_encoding)\n",
    "                negative_embeddings.append(neg_encoding)\n",
    "                m_abs_values.append(M_abs)\n",
    "\n",
    "        # Convert everything to tensors\n",
    "        query_embeddings = torch.vstack(query_embeddings)\n",
    "        positive_embeddings = torch.vstack(positive_embeddings)\n",
    "        negative_embeddings = torch.vstack(negative_embeddings)\n",
    "        relevance_score_deltas = torch.tensor(m_abs_values, dtype=torch.float32).to(\n",
    "            \"cuda\"\n",
    "        )\n",
    "\n",
    "        query_embeddings.requires_grad_(True)\n",
    "        positive_embeddings.requires_grad_(True)\n",
    "        negative_embeddings.requires_grad_(True)\n",
    "        relevance_score_deltas.requires_grad_(True)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        f_q_d_pos = torch.cosine_similarity(\n",
    "            query_embeddings, positive_embeddings, dim=1\n",
    "        )\n",
    "        f_q_d_neg = torch.cosine_similarity(\n",
    "            query_embeddings, negative_embeddings, dim=1\n",
    "        )\n",
    "\n",
    "        f_q_d_pos.requires_grad_(True)\n",
    "        f_q_d_neg.requires_grad_(True)\n",
    "\n",
    "        loss = adore_loss(f_q_d_pos, f_q_d_neg, relevance_score_deltas).mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Batch {i // batch_size + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "        if save_every_n_batches:\n",
    "            if (i // batch_size) % save_every_n_batches == 0:\n",
    "                checkpoint_path = (\n",
    "                    f\"{checkpoint_dir}/query_encoder_epoch{epoch + 1}_batch{i}.pt\"\n",
    "                )\n",
    "                print(f\"Saving checkpoint: {checkpoint_path}\")\n",
    "                adore_query_encoder.save_pretrained(checkpoint_path)\n",
    "                adore_tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "    # Save at the end of each epoch\n",
    "    checkpoint_path = f\"{checkpoint_dir}/query_encoder_epoch{epoch + 1}.pt\"\n",
    "    print(f\"Saving epoch checkpoint: {checkpoint_path}\")\n",
    "    adore_query_encoder.save_pretrained(checkpoint_path)\n",
    "    adore_tokenizer.save_pretrained(checkpoint_path)\n",
    "    adore_retriever.set_query_encoder(checkpoint_path)\n",
    "    adore_retriever.set_query_tokenizer(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation of QA Performance for ADORE\n",
    "\n",
    "Here we evaluate the performance of our model, and compare it to the `ContrieverRetriever` model. Note: make sure that all imports from the earlier parts are still available. If not, please go back to the imports and run this cell. Also check for the Cuda availability and the `HF_TOKEN` environment variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setup new retriever \n",
    "\n",
    "Also calculates some metrics for the new retriever at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.ini\"\n",
    "\n",
    "loader = RetrieverDataset(\n",
    "    \"wikimultihopqa\", \"wikimultihopqa-corpus\", config_path, Split.DEV, tokenizer=None\n",
    ")\n",
    "\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "print(f\"Loader initialized with {len(queries)} queries and {len(corpus)} documents.\")\n",
    "\n",
    "adore_config = DenseHyperParams(  # use a patched config for adore retriever\n",
    "    query_encoder_path=\"facebook/contriever\",  # we replace the query encoder later by our own\n",
    "    document_encoder_path=\"facebook/contriever\",\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Adore config: \", adore_config)\n",
    "\n",
    "# create the retriever instance, with the same config as the contriever\n",
    "adore_retriever: AdoreRetriever = AdoreRetriever(\n",
    "    config=adore_config, corpus_folder=\"indices/corpus\", corpus_file=\"index_1\"\n",
    ")\n",
    "\n",
    "checkpoint_path = \"checkpoints/query_encoder_epoch50.pt\"\n",
    "# checkpoint_path = \"checkpoints2/query_encoder_epoch2.pt\"\n",
    "adore_retriever.set_query_encoder(checkpoint_path)\n",
    "adore_retriever.set_query_tokenizer(checkpoint_path)\n",
    "\n",
    "adore_retriever_response = adore_retriever.retrieve(\n",
    "    corpus=corpus,\n",
    "    queries=queries,\n",
    "    top_k=100,\n",
    "    score_function=CosineSimilarity(),\n",
    "    chunksize=400000,\n",
    ")\n",
    "\n",
    "metrics = RetrievalMetrics(k_values=[1, 3, 5])  # Evaluate retrieval metrics\n",
    "print(metrics.evaluate_retrieval(qrels=qrels, results=adore_retriever_response))\n",
    "\n",
    "# CONTRIEVER BASELINE:\n",
    "# dev: ({'NDCG@1': 0.4225, 'NDCG@3': 0.33825, 'NDCG@5': 0.27994}, {'MAP@1': 0.0425, 'MAP@3': 0.07537, 'MAP@5': 0.08785}, {'Recall@1': 0.0425, 'Recall@3': 0.09387, 'Recall@5': 0.11961}, {'P@1': 0.4225, 'P@3': 0.31111, 'P@5': 0.23783})\n",
    "\n",
    "# ADORE:\n",
    "# 10 epochs, lr 5e-6, dev: ({'NDCG@1': 0.58583, 'NDCG@3': 0.45797, 'NDCG@5': 0.37441}, {'MAP@1': 0.05888, 'MAP@3': 0.10462, 'MAP@5': 0.12159}, {'Recall@1': 0.05888, 'Recall@3': 0.12597, 'Recall@5': 0.1572}, {'P@1': 0.58583, 'P@3': 0.4175, 'P@5': 0.31267})\n",
    "# 5 epochs, lr 5e-4, dev: ({'NDCG@1': 0.6775, 'NDCG@3': 0.49926, 'NDCG@5': 0.39481}, {'MAP@1': 0.0681, 'MAP@3': 0.11416, 'MAP@5': 0.12831}, {'Recall@1': 0.0681, 'Recall@3': 0.13408, 'Recall@5': 0.15869}, {'P@1': 0.6775, 'P@3': 0.44472, 'P@5': 0.31567})\n",
    "# 10 epochs, lr 5e-4, dev: ({'NDCG@1': 0.69833, 'NDCG@3': 0.51237, 'NDCG@5': 0.40646}, {'MAP@1': 0.07021, 'MAP@3': 0.1179, 'MAP@5': 0.13367}, {'Recall@1': 0.07021, 'Recall@3': 0.13757, 'Recall@5': 0.1637}, {'P@1': 0.69833, 'P@3': 0.45611, 'P@5': 0.32567})\n",
    "# 50 epochs, lr 5e-4, dev: ({'NDCG@1': 0.69917, 'NDCG@3': 0.54798, 'NDCG@5': 0.42863}, {'MAP@1': 0.07027, 'MAP@3': 0.12902, 'MAP@5': 0.14332}, {'Recall@1': 0.07027, 'Recall@3': 0.15009, 'Recall@5': 0.17327}, {'P@1': 0.69917, 'P@3': 0.49778, 'P@5': 0.34483})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Some configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# For any other hf model to work just use AutoModelForCausalLM instead of LLamaForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "dataset_dir = \"data\"\n",
    "dev_path = f\"{dataset_dir}/musique/dev.json\"\n",
    "corpus_path = f\"{dataset_dir}/corpus/wiki_musique_corpus.json\"\n",
    "\n",
    "with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_dict = json.load(f)\n",
    "\n",
    "# set up the text generator\n",
    "text_generator = transformers.pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "dev_dict = defaultdict(list)\n",
    "\n",
    "for item in dev_data:\n",
    "    if \"_id\" in item:\n",
    "        _id = item[\"_id\"]\n",
    "        dev_dict[_id].append(item)\n",
    "    else:\n",
    "        print(\"Warning: JSON object missing '_id' field:\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating dataset...\")\n",
    "\n",
    "dataset = []  # a list of dictionaries where each represnts a question from the dev set and related retrieved contexts\n",
    "for dev_key, retrieved_contexts in islice(\n",
    "    adore_retriever_response.items(), len(adore_retriever_response.items())\n",
    "):\n",
    "    outer_dict = defaultdict(\n",
    "        list\n",
    "    )  # the outer dictionary with dev_id, dev_full, context_list\n",
    "    outer_dict[\"dev_id\"] = dev_key\n",
    "    dev_element = dev_dict[dev_key]\n",
    "    outer_dict[\"dev_full\"] = dev_element[0]\n",
    "\n",
    "    context_list = []  # list of dictionaries where each dict has the following keys: context_id, context_score, context_full\n",
    "\n",
    "    # contexts are ordered from the best to the worst, accoring to the used retriever\n",
    "    sorted_contexts = sorted(\n",
    "        retrieved_contexts.items(), key=lambda item: item[1], reverse=True\n",
    "    )\n",
    "    for context_key, context_score in sorted_contexts:\n",
    "        context_dict = defaultdict(list)\n",
    "        context_dict[\"context_id\"] = context_key\n",
    "        context_dict[\"context_score\"] = context_score\n",
    "        context_dict[\"context_full\"] = corpus_dict[context_key]\n",
    "        context_list.append(context_dict)\n",
    "\n",
    "    outer_dict[\"context_list\"] = context_list\n",
    "\n",
    "    dataset.append(outer_dict)\n",
    "\n",
    "print(\"dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "CHECKPOINT_FOLDER = \"outputs\"\n",
    "\n",
    "dataset_copy = dataset\n",
    "\n",
    "exact_match_counts = defaultdict(int)\n",
    "exact_match_cover_counts = defaultdict(int)\n",
    "bertscore_f1_scores = defaultdict(float)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "elem_count = 0\n",
    "count_parsing_errors = 0\n",
    "\n",
    "counts = [1, 5, 15]\n",
    "print(f\"Starting run on {len(dataset_copy)} entries...\")\n",
    "\n",
    "t_start = time()\n",
    "count_iters = 0\n",
    "\n",
    "for element in dataset_copy:\n",
    "    count_iters += 1\n",
    "    elem_count += 1\n",
    "\n",
    "    if count_iters == 100:\n",
    "        t_curr = time()\n",
    "        print(f\"\\nCompleted iterations: {elem_count}\")\n",
    "        print(f\"Time ellapsed: {t_curr - t_start}\\n\")\n",
    "        count_iters = 0\n",
    "\n",
    "    dev_id = element[\"dev_id\"]\n",
    "    dev_dict = element[\"dev_full\"]\n",
    "    all_contexts = element[\"context_list\"]\n",
    "\n",
    "    question = dev_dict[\"question\"]\n",
    "    correct_answer = dev_dict[\"answer\"]\n",
    "\n",
    "    for count in counts:\n",
    "        t_count = time()\n",
    "        evidences = []\n",
    "        top_k_contexts = all_contexts[:count]\n",
    "\n",
    "        for context_item in top_k_contexts:\n",
    "            context_score = context_item[\"context_score\"]\n",
    "            context_full = context_item[\"context_full\"]\n",
    "            context_title = context_full[\"title\"]\n",
    "            context_text = context_full[\"text\"]\n",
    "            evidences.append(context_text)\n",
    "\n",
    "        prompt = prepare_prompt(question, evidences)\n",
    "        output = text_generator(prompt, max_new_tokens=50)\n",
    "\n",
    "        try:\n",
    "            model_answer = get_answer_from_model_output(output)\n",
    "            if model_answer is None or correct_answer is None:\n",
    "                print(\n",
    "                    f\"Skipping invalid example. model_answer: {model_answer}, correct_answer: {correct_answer}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            EM_score = exact_match_score(model_answer, correct_answer)\n",
    "            cover_score = cover_exact_match_score(model_answer, correct_answer)\n",
    "            P, R, F1 = score([model_answer], [correct_answer], lang=\"en\")\n",
    "            bert_f1 = F1.mean().item()\n",
    "\n",
    "            bertscore_f1_scores[count] += bert_f1\n",
    "            exact_match_counts[count] += EM_score\n",
    "            exact_match_cover_counts[count] += cover_score\n",
    "        except JSONDecodeError:\n",
    "            # print(\"\\nExtracted answer was null!.\")\n",
    "            # pprint(output)\n",
    "            count_parsing_errors += 1\n",
    "\n",
    "for count in counts:\n",
    "    bertscore_f1_scores[count] /= elem_count\n",
    "    exact_match_counts[count] /= elem_count\n",
    "    exact_match_cover_counts[count] /= elem_count\n",
    "\n",
    "pprint(exact_match_counts)\n",
    "pprint(exact_match_cover_counts)\n",
    "pprint(bertscore_f1_scores)\n",
    "print(f\"decoding errors: {count_parsing_errors}\")\n",
    "\n",
    "t_end = time()\n",
    "total_time = t_end - t_start\n",
    "\n",
    "print(\"Total elapsed time: \", total_time)\n",
    "\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_counts,\n",
    "    title=\"Exact Match Performance - ADORE\",\n",
    "    save_path=\"plots/adore_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    exact_match_cover_counts,\n",
    "    title=\"Cover Exact Match Performance - ADORE\",\n",
    "    save_path=\"plots/adore_cover_exact_match.png\",\n",
    ")\n",
    "plot_accuracy_bar_chart(\n",
    "    bertscore_f1_scores,\n",
    "    title=\"Bert score Performance - ADORE\",\n",
    "    save_path=\"plots/adore_bertscore.png\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
